---
title: "Classifying Weight Lifting Exercises"
author: "Javier Prado"
date: "October 10, 2017"
output: html_document
---

```{r echo=FALSE, message=FALSE, cache=TRUE}
library(caret); library(pROC); library(parallel); library(doParallel)
train_link <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_link  <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
```

<style>
body {
text-align: justify}
</style>

<img style="float: right; margin: 0px 0px 0px 20px" src="http://groupware.les.inf.puc-rio.br/static/WLE/on-body-sensing-schema.png" width="200" height="200">
<br>
The Weight Lifting Dataset used in [this study](http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf) contains motion metrics of 6 individuals performing a specific weight lifting exercise (barbell lifts) in 5 different ways: 1 correct way according to specification and 4 incorrect.

An initial exploration of the data will narrow down the feature space and test predictors' distributions for normality in order to assess the convenience of using PCA preprocessing and Discriminant Analysis modeling.

Finally, we'll predict the 5 different ways this exercise can be done using 2 off-the-shelf classification models:

* **Multinomial regression** (*generalization of logistic regression for multiclass problems*)
    + Regularization parameter: **decay** (*Weight Decay*)

* **Random Forest**
    + Regularization parameter: **mtry** (*Number of Randomly Selected Predictors at each node*)

<br>

Due to performance constraints, 5-fold cross-validation will be used across the models to tune the regularization parameters and to estimate out-of-sample error, to then choose the model with highest accuracy.
```{r echo=FALSE, cache=TRUE}
train_original <- read.csv(train_link, na.strings = c("NA","#DIV/0!"))
# "#DIV/0!" included as NA value 
```

<br>

##Dimensionality reduction exploration

The original study made use of a sliding window that aggregated subsequent sensor readings and generated 96 new features for every window out of the total 52 raw features. Only the raw sensor readings will be used in our modeling, eliminating the ones generated by the sliding window technique. `timestamp` & `window` features will be discarded. To avoid further complexity, `username` information will also be discarded.

```{r echo=FALSE, cache=TRUE}
train_set <- train_original[,colSums(is.na(train_original))==0]
train_set <- train_set[,-(1:7)]
# Window-aggregated and other selected features removed
```

This initial feature selection leaves us with `r ncol(train_set)` variables: 52 continuous predictors and 1 categorical outcome. These numerical predictors and categorical outcome hint us at using a Discriminant Analysis approach to generate a model, unless LDA's assumption about normality of predictors' distributions for each class can't be met. That is $P(\vec{x}|y)$ is Gaussian for every $x$ in predictors and every $y$ in type of outcome.

```{r echo=FALSE, cache=TRUE}
classes <- c("A","B","C","D","E")
results <- matrix(nrow=5, ncol=52)
for(i in 1:5){
   for(j in 1:52){ 
           #R's implementation of the test doesn't allow more than 5000 values to be tested
           s <- train_set[which(train_set$classe==classes[i]),j]
           set.seed(23)
           results[i,j] <- ifelse(length(s)>5000, shapiro.test(sample(s,5000))$p.value, shapiro.test(s)$p.value)
   }
}
```

```{r echo=FALSE, fig.align="center", fig.width=9, fig.height=2.7, cache=TRUE}
rownames(results) <- classes
colnames(results) <- colnames(train_set)[-53]

heatmap <- reshape2::melt(results)
heatmap$Var1 = with(heatmap, factor(Var1, levels = rev(levels(Var1))))

ggplot(data = heatmap, aes(x = Var2, y = Var1)) + geom_tile(aes(fill = value),color="gray") + theme(panel.background = element_blank(), axis.text.x = element_text(angle = 60, hjust = 1), plot.title = element_text(hjust = 0.5)) + labs(x=NULL, y=NULL) + ggtitle("Shapiro-Wilk normality test of predictors for each class")
```

* The null hypothesis for the Shapiro-Wilk test is that the variable is normally distributed.
* Predictors for each class that fail to reject the null hyp (p-value >= 0.05) are **`r ifelse(any(results>=.05),which(results>=.05),"0")`**.
* We can safely reject the hypothesis of normality and, therefore, the use of Discriminant Analysis models.

There's another consequence derived from this non-normality that we just assessed: PCA preprocessing might miss higher order statistics beyond variance that are not taken into account by PCA, as it best captures variance when the data are normally distributed. Our 2 models will be applied off-the-shelf with basic centering and scaling of the 52 raw sensor features.

<br>

##Model selection

As introduced earlier, the 2 models will be trained and cross-validated with 5 data folds. Different parameter settings will be tuned through this cross-validation and a final model decision will be made based on accuracy values:

```{r echo=FALSE, cache=TRUE}
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
```
```{r message=FALSE, results=FALSE, cache=TRUE}
fitControl <- trainControl(method = "cv", number = 5, allowParallel = TRUE)

set.seed(23)
fit1 <- train(classe~., method="multinom",data=train_set, preProcess=c("center","scale"), trControl = fitControl)

set.seed(7)
fit2 <- train(classe~., method="rf"      ,data=train_set, preProcess=c("center","scale"), trControl = fitControl)
```

<br>

* **Multinomial Regression results:**
```{r echo=FALSE, cache=TRUE}
knitr::kable(fit1$results)
```

* **Random Forest results:**
```{r echo=FALSE, cache=TRUE}
knitr::kable(fit2$results)
```

The accuracy of Random Forest modeling stands out. Even under performance constrains, we were able to reach a 99% accuracy for the best tuned model. Further visualizations of the results can be found in Appendix 1.

<br>

##Final model and test prediction

Our final model performs the following steps on the test set:
```{r cache=TRUE}
test_original  <- read.csv(test_link, na.strings = c("NA","#DIV/0!")) # "#DIV/0!" included as NA value
test_set <- test_original[,colSums(is.na(train_original))==0] # Eliminate features containing NA values
test_set <- test_set[,-(1:7)] # Eliminate username and window derived features

predict(fit2, newdata=test_set[,1:52]) # Predict on the 52 raw sensor features
```

<br>

###Appendix 1
####*Parameter tuning*
```{r echo=FALSE, cache=TRUE, fig.align="center"}
plot(fit1)
```
```{r echo=FALSE, cache=TRUE, fig.align="center"}
plot(fit2)
```

```{r echo=F}
print("Javier Prado")
```

